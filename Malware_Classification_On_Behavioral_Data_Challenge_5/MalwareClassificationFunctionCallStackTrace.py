##malwares were given as their md5 signature, stack trace of function calls was given as a feature(text) to each malware and
##we need to classify malware into categories(1 to 10)

import pandas as pd
import numpy as np
import os
os.chdir('C:\\Users\\priyagoe\\Downloads\\Anomaly-Detection-practical-master\\Anomaly-Detection-practical-master\\Challenges\\Malware_Classification_On_Behavioral_Data_Challenge_5\\')

##Reading Data

Xtrain = pd.read_csv('training_data.csv')
XTrain = Xtrain.as_matrix()
print (XTrain[0])
# XTest = np.loadtxt('test_data_public_new.csv', skiprows=1, usecols=(1,), delimiter=',',dtype='str')
# print XTest[0]
YTrain = XTrain[:,-1]
XTrain = XTrain[:,0]
print (XTrain.shape)
YTrain = YTrain.astype(np.int)
print (type(YTrain[0]),YTrain[0],YTrain.shape)
# print XTest.shape

print (XTrain, YTrain)
def readInputFile(inputFile):
    with open(inputFile,'r') as f:
        fileContent = f.readlines()
        f = [i.strip('\n') for i in fileContent]
        return ' '.join(f)

def readFolder(folder, fileNames):
    path = folder
    folderContents = []
    for f in fileNames:
        folderContents.append(readInputFile(os.path.join(folder, f+'.txt')))
    return folderContents
XTrain = [str(i) for i in range(2042)]
trainSamples = readFolder(os.getcwd()+os.sep+'train', XTrain)
# print len(trainSamples), trainSamples[:2]
XTest = [str(i) for i in range(706)]
testSamples = readFolder(os.getcwd()+os.sep +'test', XTest)
# print len(testSamples), testSamples[:2]

##Feature Engineering

###Bag of words as feature
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
XTr = vectorizer.fit_transform(trainSamples)
print (len(vectorizer.get_feature_names()))
trainBagVector = XTr.toarray()
print (trainBagVector.shape)
XTe = vectorizer.transform(testSamples)
testBagVector = XTe.toarray()
print (testBagVector.shape)


##TF IDF vector as feature
from sklearn.feature_extraction.text import TfidfTransformer
transformer = TfidfTransformer()
# print transformer
tfidfTrain = transformer.fit_transform(trainBagVector)
tfidfTrain = tfidfTrain.toarray()
tfidfTest = transformer.fit_transform(testBagVector)
tfidfTest = tfidfTest.toarray()
print (tfidfTrain.shape, tfidfTest.shape)
print (tfidfTrain[0])
print (tfidfTest[0])


##Model Definition, prediction and accuracy

# ##Hmm
# from hmmlearn.hmm import GaussianHMM
#
# XTrainHMM = trainBagVector
# XTestHMM = testBagVector
#
# # Make an HMM instance and fitting to HMM
# model = hmmm.GaussianHMM(n_components=10, covariance_type="diag", n_iter=1000).fit(XTrainHMM)
#
# # Predict the optimal sequence of internal hidden state
# hidden_states = model.predict(XTestHMM)

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
# def classifyRandomForestClassifier(XTrain, XTest, YTrain, YTest,trees=100,crit='gini'):
def classifyRandomForestClassifier(XTrain, XTest, YTrain, YTest, params):
    trees = params['trees']
    crit = params['criterion']
    seed = params['random_state']
    clf = RandomForestClassifier(n_estimators=trees,criterion=crit,random_state=seed)
    clf.fit(XTrain, YTrain)
    YPred = clf.predict(XTest)
    diff = YPred - YTest
    score = diff[diff == 0].size
    return (100.0 * score)/(YPred.size)


#Multi Class SVM
from sklearn import svm
def classifyMultiClassSVMClassifier(XTrain, XTest, YTrain, YTest, params):
    ker = params['kernel']
    YPred = svm.SVC(kernel=ker).fit(XTrain, YTrain).predict(XTest)
    diff = YPred - YTest
    score = diff[diff == 0].size
    return (100.0 * score)/(YPred.size)

#K Nearest Neighbours Classifier
from sklearn.neighbors import KNeighborsClassifier
def classifyKNNClassifier(XTrain, XTest, YTrain, YTest, params):
#     print XTrain.shape, XTest.shape
    neighbours = params['neighbours']
    neigh = KNeighborsClassifier(n_neighbors=neighbours)
    YPred = neigh.fit(XTrain, YTrain).predict(XTest)
    diff = YPred - YTest
    score = diff[diff == 0].size
    return (100.0 * score)/(YPred.size)


# Logistic Regression
from sklearn import linear_model
def classifyLogisticRegression(XTrain, XTest, YTrain, YTest, params):
    LogReg = linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)
    LogReg.fit(XTrain, YTrain)

    YPred = LogReg.predict(XTest)
    diff = YPred - YTest
    score = diff[diff == 0].size
    return (100.0 * score)/(YPred.size)

# Adaboost Classfier
from sklearn.ensemble import AdaBoostClassifier

from sklearn.tree import DecisionTreeClassifier
def classifyAdaboostClassifier(XTrain, XTest, YTrain, YTest, params):


    # Create and fit an AdaBoosted decision tree
    bdt = AdaBoostClassifier(DecisionTreeClassifier())

    bdt.fit(XTrain, YTrain)
    YPred = bdt.predict(XTest)

    diff = YPred - YTest
    score = diff[diff == 0].size
    return (100.0 * score) / (YPred.size)


# Neural Networks
try:
    from sknn.mlp import Classifier, Layer
except ImportError:
    print ('Please install scikit-neuralnetwork(pip install scikit-neuralnetwork)')

def classifyNeuralNetworkClassifier(XTrain, XTest, YTrain, YTest, params):
    activation = params['activation']
    actLastLayer = params['actLastLayer']
    rule = params['rule']
    noOfUnits = params['units']
    rate = params['rate']
    noOfIter = params['iter']
    nn = Classifier(layers=[Layer(activation, units=noOfUnits),Layer(actLastLayer)], learning_rule=rule,
        learning_rate=0.02,
        n_iter=10)
    nn.fit(XTrain, YTrain)
    YPred = nn.predict(XTest)
    diff = YPred - YTest.reshape(YPred.shape)
    score = diff[diff == 0].size
    score = (100.0 * score)/(YPred.size)
    return score

##Stratified K Fold Cross Validation
from sklearn.cross_validation import StratifiedKFold


def stratifiedKFoldVal(XTrain, YTrain, classify, params):
    n_folds = 5
    score = 0.0
    skf = StratifiedKFold(YTrain, n_folds)
    try:
        multi = params['multi']
    except KeyError:
        multi = False
    for train_index, test_index in skf:
        y_train, y_test = YTrain[train_index], YTrain[test_index]
        if not multi:
            X_train, X_test = XTrain[train_index], XTrain[test_index]
            score += classify(X_train, X_test, y_train, y_test, params)
        else:
            X_train, X_test = [XTrain[i] for i in train_index], [XTrain[i] for i in test_index]
            score += classify(np.array(X_train), np.array(X_test), y_train, y_test, params)
    print('classify')
    print(classify)
    print(score / n_folds)
    return score / n_folds

##Calling Different models first with bag of words as vector then with TF IDF vector

def CallingClassifiers():
    params = {'trees': 150, 'criterion': 'entropy', 'random_state': None, 'kernel': 'linear', 'activation': 'Tanh',
              'actLastLayer': 'Softmax', 'rule': 'adagrad', 'units': 100, 'rate': 0.002, 'iter': 10, 'neighbours' : 20 }
    print('classifyNeuralNetworkClassifier')
    stratifiedKFoldVal(trainBagVector, YTrain, classifyNeuralNetworkClassifier, params)
    print('classifyAdaboostClassifier')
    stratifiedKFoldVal(trainBagVector, YTrain, classifyAdaboostClassifier, params)
    print('classifyLogisticRegression')
    stratifiedKFoldVal(trainBagVector, YTrain, classifyLogisticRegression, params)
    print('classifyKNNClassifier')
    stratifiedKFoldVal(trainBagVector, YTrain, classifyKNNClassifier, params)
    print('classifyMultiClassSVMClassifier')
    stratifiedKFoldVal(trainBagVector, YTrain, classifyMultiClassSVMClassifier, params)
    print('classifyRandomForestClassifier')
    stratifiedKFoldVal(trainBagVector, YTrain, classifyRandomForestClassifier, params)

    print('With TF IDF')
    print('classifyNeuralNetworkClassifier')
    stratifiedKFoldVal(tfidfTrain, YTrain, classifyNeuralNetworkClassifier, params)
    print('classifyAdaboostClassifier')
    stratifiedKFoldVal(tfidfTrain, YTrain, classifyAdaboostClassifier, params)
    print('classifyLogisticRegression')
    stratifiedKFoldVal(tfidfTrain, YTrain, classifyLogisticRegression, params)
    print('classifyKNNClassifier')
    stratifiedKFoldVal(tfidfTrain, YTrain, classifyKNNClassifier, params)
    print('classifyMultiClassSVMClassifier')
    stratifiedKFoldVal(tfidfTrain, YTrain, classifyMultiClassSVMClassifier, params)
    print('classifyRandomForestClassifier')
    stratifiedKFoldVal(tfidfTrain, YTrain, classifyRandomForestClassifier, params)

##Normalizing Featurs
from sklearn import preprocessing
def NormalizeVector(XTestFeatures,XTrainFeatures):
    XTestFeaturesNorm = preprocessing.normalize(XTestFeatures, norm='l2')
    XTrainFeaturesNorm = preprocessing.normalize(XTrainFeatures, norm='l2')
    print (XTrainFeaturesNorm.shape,XTestFeaturesNorm.shape)
#     print XTrainFeaturesNorm[0],XTestFeaturesNorm[0]
    return XTrainFeaturesNorm, XTestFeaturesNorm

## Classifying using Nltkfeature vectors , it has functions for stemming, tagging data

import nltk
##get features in format nltk can classify
def featNLTKClassify(samples, phase):
    featureVectors = vectorizer.get_feature_names()
    nltkClassifySamples = []

    for i in range(len(samples)):
        t = samples[i]
        lstFuncCalls = t.split()
        wordOccDict = {}
        for j in range(len(featureVectors)):
             wordOccDict[featureVectors[j]] = lstFuncCalls.count(featureVectors[j])
        if phase == 'train':
            nltkClassifySamples.append((wordOccDict, YTrain[i]))
        else:
            nltkClassifySamples.append(wordOccDict)

    return nltkClassifySamples

##NaiveBayesClassifier
from nltk.classify import naivebayes
nltkClassifyTrain = featNLTKClassify(trainSamples, 'train')
nltkClassifyTest = featNLTKClassify(testSamples, 'test')

tr = nltkClassifyTrain
te = nltkClassifyTest
classifier = nltk.classify.NaiveBayesClassifier.train(tr)
sorted(classifier.labels())


classifier.classify_many(te)

classifier.show_most_informative_features()


##maxm entropy classifier
from nltk.classify import maxent
tr = nltkClassifyTrain
te = nltkClassifyTest
classifierME = maxent.MaxentClassifier.train(tr, bernoulli=False, encoding='encoding', trace=0)
classifierME.classify_many(te)

##decsion tree nltk classifier
tr = nltkClassifyTrain
te = nltkClassifyTest

classifier = nltk.classify.DecisionTreeClassifier.train(tr, entropy_cutoff=0,support_cutoff=0)
sorted(classifier.labels())
print(classifier)
classifier.classify_many(te)


##KNN with TF- IDF with k = 5, gave best results, Random forest with Bag of words gave best results.